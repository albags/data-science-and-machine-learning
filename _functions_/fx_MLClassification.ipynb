{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_train_Class(model, X_train, y_train):\n",
    "    scoring = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_macro': 'recall_macro',\n",
    "           'f1_macro': 'f1_macro'}\n",
    "    \n",
    "    scores = cross_validate(model, X_train, y_train, cv=10, scoring=scoring)\n",
    "    ypredTrain = model.predict(X_train)\n",
    "    Acc_train = scores['test_acc'].mean()\n",
    "    Precision_train = scores['test_prec_macro'].mean()\n",
    "    Recall_train = scores['test_rec_macro'].mean()\n",
    "    F1_train = scores['test_f1_macro'].mean()\n",
    "    conf_matrix_train = confusion_matrix(y_train, ypredTrain)\n",
    "    from sklearn.metrics import classification_report\n",
    "    statist_train = []\n",
    "   \n",
    "    list_metrics = [Acc_train, Precision_train, Recall_train, F1_train]\n",
    "    statist_train.append(list_metrics)\n",
    "    statist_train = pd.DataFrame(statist_train,columns = ['Accuracy', 'Precision', 'Recall', 'f1'], index = ['Train'])\n",
    "    \n",
    "    print('-----------------------------------------')\n",
    "    print('TRAIN results')\n",
    "    print('-----------------------------------------')\n",
    "    print('Confusion Matrix \\n', conf_matrix_train)\n",
    "    return statist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_test_Class(model, X_test, y_test):\n",
    "    \n",
    "    scoring = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_macro': 'recall_macro',\n",
    "           'f1_macro': 'f1_macro'}\n",
    "    \n",
    "    scores = cross_validate(model, X_test, y_test, \n",
    "                            cv=10, scoring=scoring)\n",
    "    ypredtest = model.predict(X_test)\n",
    "    report = classification_report(y_test, ypredtest,zero_division=0, output_dict=True)\n",
    "    report = pd.DataFrame(report).T\n",
    "    \n",
    "    Acc_test = report.loc['accuracy', :].mean()  \n",
    "    Rest_metrics = report.iloc[:-3,:]\n",
    "    \n",
    "    Precision_test = Rest_metrics.loc[:,'precision'].mean()\n",
    "    Recall_test = Rest_metrics.loc[:,'recall'].mean()\n",
    "    F1_test = Rest_metrics.loc[:,'f1-score'].mean()\n",
    "    conf_matrix_test = confusion_matrix(y_test, ypredtest)\n",
    "    \n",
    "    statist_test = []\n",
    "   \n",
    "    list_metrics = [Acc_test, Precision_test, Recall_test, F1_test]\n",
    "    statist_test.append(list_metrics)\n",
    "    statist_test = pd.DataFrame(statist_test,columns = ['Accuracy', 'Precision', 'Recall', 'f1'], index = ['test'])\n",
    "     \n",
    "    print('-----------------------------------------')\n",
    "    print('TEST results')\n",
    "    print('-----------------------------------------')\n",
    "    print('Confusion Matrix \\n', conf_matrix_test)\n",
    "    print(' Classification report \\n', Rest_metrics)\n",
    "    return statist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Allmetrics(model,X_train,y_train,X_test,y_test):\n",
    "    \n",
    "    stats_train = metrics_train_Class(model, X_train,y_train)\n",
    "    stats_test = metrics_test_Class(model, X_test,y_test)\n",
    "    final_metrics = pd.concat([stats_train,stats_test])\n",
    "    print()\n",
    "    print('++++++++ Summary of the Metrics +++++++++++++++++++++++++++++++++++')\n",
    "    print(final_metrics)\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
